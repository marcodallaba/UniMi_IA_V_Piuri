\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{italian}{}
\babel@aux{italian}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduzione}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Neurone\relax }}{2}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:1}{{1}{2}{Neurone\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Reti neurali}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Background biologico}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Threshold logic unit}{3}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Rappresentazione geometrica della TLU per $x_1 \land x_2$\relax }}{4}{figure.caption.3}}
\newlabel{fig:2}{{2}{4}{Rappresentazione geometrica della TLU per $x_1 \land x_2$\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Interpretazione geometrica}{4}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces La doppia implicazione non \IeC {\`e} linearmente separabile\relax }}{5}{figure.caption.4}}
\newlabel{fig:3}{{3}{5}{La doppia implicazione non è linearmente separabile\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces network di TLU che simula la doppia implicazione\relax }}{5}{figure.caption.5}}
\newlabel{fig:4}{{4}{5}{network di TLU che simula la doppia implicazione\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Training delle TLU}{5}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces funzione di errore per la negazione booleana\relax }}{6}{figure.caption.6}}
\newlabel{fig:5}{{5}{6}{funzione di errore per la negazione booleana\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces funzione di errore differenziabile\relax }}{6}{figure.caption.7}}
\newlabel{fig:6}{{6}{6}{funzione di errore differenziabile\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Artificial neural network}{7}{subsection.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces rappresentazione di un singolo neurone\relax }}{8}{figure.caption.8}}
\newlabel{fig:7}{{7}{8}{rappresentazione di un singolo neurone\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces computazione di una recurrent neural network che non giunge ad uno stato stabile\relax }}{8}{figure.caption.9}}
\newlabel{fig:8}{{8}{8}{computazione di una recurrent neural network che non giunge ad uno stato stabile\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Training delle ANN}{9}{subsection.2.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces multi-layer perceptrons\relax }}{10}{figure.caption.10}}
\newlabel{fig:9}{{9}{10}{multi-layer perceptrons\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Multi-layer perceptrons}{10}{subsection.2.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Alcune funzioni sigmoidi\relax }}{11}{figure.caption.11}}
\newlabel{fig:10}{{10}{11}{Alcune funzioni sigmoidi\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Approssimazione di una funzione continua con una step function\relax }}{12}{figure.caption.12}}
\newlabel{fig:11}{{11}{12}{Approssimazione di una funzione continua con una step function\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces MLP che calcola la step function in Figura \ref  {fig:11}\relax }}{12}{figure.caption.13}}
\newlabel{fig:12}{{12}{12}{MLP che calcola la step function in Figura \ref {fig:11}\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Regressione}{12}{subsection.2.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Regressione lineare}{13}{subsubsection.2.8.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Regressione polinomiale e multilineare}{13}{subsubsection.2.8.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.3}Regressione logistica}{14}{subsubsection.2.8.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Backpropagation}{14}{subsection.2.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Il gradiente di una funzione a due argomenti.\relax }}{15}{figure.caption.14}}
\newlabel{fig:13}{{13}{15}{Il gradiente di una funzione a due argomenti.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Propagazione dell'errore in un MLP.\relax }}{16}{figure.caption.15}}
\newlabel{fig:14}{{14}{16}{Propagazione dell'errore in un MLP.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Variazioni sul gradient descent}{16}{subsubsection.2.9.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}Overfitting e underfitting}{17}{subsubsection.2.9.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.3}Sensitivity analysis}{18}{subsubsection.2.9.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.10}Deep learning}{18}{subsection.2.10}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Funzioni di attivazione sempre crescenti.\relax }}{19}{figure.caption.16}}
\newlabel{fig:15}{{15}{19}{Funzioni di attivazione sempre crescenti.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.11}Radial basis function network}{20}{subsection.2.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Cerchi rispetto alle diverse definizioni di distanza.\relax }}{21}{figure.caption.17}}
\newlabel{fig:16}{{16}{21}{Cerchi rispetto alle diverse definizioni di distanza.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Varie funzioni di attivazione per un RBFN.\relax }}{21}{figure.caption.18}}
\newlabel{fig:17}{{17}{21}{Varie funzioni di attivazione per un RBFN.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces RBFN che calcola la congiunzione booleana.\relax }}{22}{figure.caption.19}}
\newlabel{fig:18}{{18}{22}{RBFN che calcola la congiunzione booleana.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.12}Training delle RBFN}{22}{subsection.2.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.13}Learning vector quantization}{23}{subsection.2.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Attraction rule e repulsion rule in azione.\relax }}{24}{figure.caption.20}}
\newlabel{fig:19}{{19}{24}{Attraction rule e repulsion rule in azione.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.13.1}Learning vector quantization network}{24}{subsubsection.2.13.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Learing rate costante (a sinistra) e decrescente (a destra).\relax }}{25}{figure.caption.21}}
\newlabel{fig:20}{{20}{25}{Learing rate costante (a sinistra) e decrescente (a destra).\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.14}Self-organizing maps}{26}{subsection.2.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Due esempi di griglie che rappresentano una relazione di vicinato tra neuroni di output: le linee scure rappresentano i neuroni pi\IeC {\`u} vicini, mentre quelle pi\IeC {\`u} chiare rappresentano le regioni in cui viene diviso lo spazio.\relax }}{27}{figure.caption.22}}
\newlabel{fig:21}{{21}{27}{Due esempi di griglie che rappresentano una relazione di vicinato tra neuroni di output: le linee scure rappresentano i neuroni più vicini, mentre quelle più chiare rappresentano le regioni in cui viene diviso lo spazio.\relax }{figure.caption.22}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.15}Hopfield network}{27}{subsection.2.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.16}Boltzmann machines}{28}{subsection.2.16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.16.1}Training}{29}{subsubsection.2.16.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.16.2}Restricted Boltzmann machines}{30}{subsubsection.2.16.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Sistemi fuzzy}{30}{section.3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Algoritmi evolutivi}{30}{section.4}}
